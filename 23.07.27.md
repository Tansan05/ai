# 오늘 배운 것

- 경사하강법(Gradient descent)
- Newton’s Method (Newton–Raphson, 뉴턴 랩슨 method)
- 선형회귀 (for linear regression)


## 경사하강법

### 경사하강법이 모델 학습에 쓰이는 이유

우리는 보통 함수의 최솟값을 찾고자 할 때, 미분계수를 구함으로써 찾곤 한다(컴퓨터를 사용하는 것이 아닌 그냥 수학 문제를 풀 때). 컴퓨터에서 해당 방법을 사용하지 않는 이유는 다음과 같다.

1. 실제 분석에서(특히, 딥러닝 알고리즘을 활용하는 경우) 보게 되는 함수들은 형태가 굉장히 복잡해서 미분계수와 그 근을 계산하기 어려운 경우가 많기 때문에
    
2. 미분계수 계산 과정을 컴퓨터로 구현하는 것보다, 경사하강법을 구현하는 것이 훨씬 쉽기 때문에
    
3. 데이터의 양이 매우 큰 경우 경사하강법과 같은 순차적인 방법이 계산량 측면에서 훨씬 효율적이기 때문에

> 목적함수: J(θ₀ ,θ₁)
> 목표 : min J(θ₀ ,θ₁)
>          θ₀ ,θ₁ 

### 알고리즘

- 랜덤값으로 매개변수값을 설정  θ₀ ,θ₁
- θ₀ ,θ₁ 을 업데이트 
      목적함수가 최소가 될 때까지

### Convex function

>일반적으로 아래로 볼록함수
>* Concave 함수는 위로 볼록한 함수

- 지역 최소점(Local minma) 
- 전역 최소점(Global minimum)

>경사하강법 알고리즘의 시작 위치는 매번 랜덤하기 때문에 어떤 경우에는 지역 최소점에 빠져 전역 최소점(global minimum)에 도달하지 못하는 경우가 생긴다.

### Gradient descent algorithm
> 식은 ppt참고
- [Gradient descent algorithm](https://docs.google.com/presentation/d/1zfXCR1ZyKV-DKstymQ7ahv5MrkFORZWA/edit#slide=id.p63)

- 학습률이 너무 작으면 최저점으로 수렴하는데 너무 많은 시간이 걸리고 

- 학습률이 너무 크면 발산해서 최저점에 도달하지 못함.

### 종류

- #Batch_gradient_descent(BGD) 전체 데이터 셋에 대한 에러를 구한 뒤 기울기를 한번만 계산하여 모델의 parameter를 업데이트 하는 방법, batch 는 말 그대로 **total training dataset** 을 의미
- #Stochastic_Gradient_Descent(SGD)추출된 **데이터 한 개**에 대해서 error gradient 를 계산하고, Gradient descent 알고리즘을 적용
- #Mini-batch_gradient_descent(MSGD)정해진 크기의 **데이터 여러 개**에 대해서 error gradient 를 계산하고, Gradient descent 알고리즘을 적용

>teration (step) : 1-epoch를 마치는데 필요한 미니배치 개수
>Epoch (에포크) : 전체 데이터셋을 학습한 횟수

### #Batch_gradient_descent (BGD)
-  BGD은 한 스텝에 전체 데이터를 이용하기 때문에 **연산량은 많지만 업데이트 횟수가 적음** (1 epoch 당 1회 update)
-  최적해에 대한 수렴이 안정적으로 진행
-  local optimal 상태가 되면 빠져나오기 힘듦
-  parameter 업데이트를 위해 모든 학습 데이터를 고려해야 하므로 **데이터가 큰 경우 많은 메모리가 필요**. 연산이 오래 걸림

### #Stochastic_Gradient_Descent(SGD)
- 매 반복마다 다뤄야 할 데이터가 줄어들었고, 학습 속도가 빠르다는 장점
- 메모리 소모량이 매우 낮으며, 큰 데이터 셋이라 할지라도 학습이 가능
- 진폭이 크고 배치 경사 하강법보다 불안정. 손실 함수가 최소값에 가는 과정이 불안정하다 보니 최적해 (global minimum)에 정확히 도달하지 못할 가능성. 배치 경사 하강법과 반대로 local minimum에 빠지더라도 쉽게 빠져나올 수 있음.
- 데이터를 하나씩 처리하기 때문에 오차율이 크고, GPU의 성능을 전부 활용할 수 없음


## Newton’s Method (Newton–Raphson, 뉴턴 랩슨 method)
>PPT참고
- [Newton’s Method](https://docs.google.com/presentation/d/1zfXCR1ZyKV-DKstymQ7ahv5MrkFORZWA/edit#slide=id.p87)


# 어려웠던점

# 보안

# 좋았던 점

**새로운 개념에 들어가기전 필요한 개념을 다시 한번 보고 들어가 이해하기가 쉬웠다.**

# 그 외 건의사항
